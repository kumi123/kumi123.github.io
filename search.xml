<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>机器学习one</title>
      <link href="/2020/04/29/ji-qi-xue-xi-one/"/>
      <url>/2020/04/29/ji-qi-xue-xi-one/</url>
      
        <content type="html"><![CDATA[<hr><p>BGD、SGD、Mini-batch GD，前面均已讨论过，这里介绍一下Online GD。</p><p>Online GD于Mini-batch GD/SGD的区别在于，所有训练数据只用一次，然后丢弃。这样做的优点在于可预测最终模型的变化趋势。</p><p>Online GD在互联网领域用的较多，比如搜索广告的点击率（CTR）预估模型，网民的点击行为会随着时间改变。用普通的BGD算法（每天更新一次）一方面耗时较长（需要对所有历史数据重新训练）；另一方面，无法及时反馈用户的点击行为迁移。而Online GD算法可以实时的依据网民的点击行为进行迁移。</p><h2 id="2-14-线性判别分析（LDA）"><a href="#2-14-线性判别分析（LDA）" class="headerlink" title="2.14 线性判别分析（LDA）"></a>2.14 线性判别分析（LDA）</h2><h3 id="2-14-1-LDA思想总结"><a href="#2-14-1-LDA思想总结" class="headerlink" title="2.14.1 LDA思想总结"></a>2.14.1 LDA思想总结</h3><p>线性判别分析（Linear Discriminant Analysis，LDA）是一种经典的降维方法。和主成分分析PCA不考虑样本类别输出的无监督降维技术不同，LDA是一种监督学习的降维技术，数据集的每个样本有类别输出。  </p><p>LDA分类思想简单总结如下：  </p><ol><li>多维空间中，数据处理分类问题较为复杂，LDA算法将多维空间中的数据投影到一条直线上，将d维数据转化成1维数据进行处理。  </li><li>对于训练数据，设法将多维数据投影到一条直线上，同类数据的投影点尽可能接近，异类数据点尽可能远离。  </li><li>对数据进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。  </li></ol><p>如果用一句话概括LDA思想，即“投影后类内方差最小，类间方差最大”。</p><h3 id="2-14-2-图解LDA核心思想"><a href="#2-14-2-图解LDA核心思想" class="headerlink" title="2.14.2 图解LDA核心思想"></a>2.14.2 图解LDA核心思想</h3><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="//youwebsite.com/2020/04/29/ji-qi-xue-xi-one/2.21.1.5.png" alt="图1"></p><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="//youwebsite.com/2020/04/29/ji-qi-xue-xi-one/2.21.1.5.png" alt="图2"></p><h3 id="2-14-3-二类LDA算法原理"><a href="#2-14-3-二类LDA算法原理" class="headerlink" title="2.14.3 二类LDA算法原理"></a>2.14.3 二类LDA算法原理</h3><p>输入：数据集 $D={(\boldsymbol x_1,\boldsymbol y_1),(\boldsymbol x_2,\boldsymbol y_2),…,(\boldsymbol x_m,\boldsymbol y_m)}$，其中样本 $\boldsymbol x_i $ 是n维向量，$\boldsymbol y_i  \epsilon {0, 1}$，降维后的目标维度 $d$。定义</p><p>$N_j(j=0,1)$ 为第 $j$ 类样本个数；</p><p>$X_j(j=0,1)$ 为第 $j$ 类样本的集合；</p><p>$u_j(j=0,1)$ 为第 $j$ 类样本的均值向量；</p><p>$\sum_j(j=0,1)$ 为第 $j$ 类样本的协方差矩阵。</p><p>其中<br>$$<br>u_j = \frac{1}{N_j} \sum_{\boldsymbol x\epsilon X_j}\boldsymbol x(j=0,1)，<br>\sum_j = \sum_{\boldsymbol x\epsilon X_j}(\boldsymbol x-u_j)(\boldsymbol x-u_j)^T(j=0,1)<br>$$<br>假设投影直线是向量 $\boldsymbol w$，对任意样本 $\boldsymbol x_i$，它在直线 $w$上的投影为 $\boldsymbol w^Tx_i$，两个类别的中心点 $u_0$, $u_1 $在直线 $w$ 的投影分别为 $\boldsymbol w^Tu_0$ 、$\boldsymbol w^Tu_1$。</p><p>LDA的目标是让两类别的数据中心间的距离 $| \boldsymbol w^Tu_0 - \boldsymbol w^Tu_1 |^2_2$ 尽量大，与此同时，希望同类样本投影点的协方差$\boldsymbol w^T \sum_0 \boldsymbol w$、$\boldsymbol w^T \sum_1 \boldsymbol w$ 尽量小，最小化 $\boldsymbol w^T \sum_0 \boldsymbol w - \boldsymbol w^T \sum_1 \boldsymbol w$ 。<br>定义<br>类内散度矩阵<br>$$<br>S_w = \sum_0 + \sum_1 =<br>    \sum_{\boldsymbol x\epsilon X_0}(\boldsymbol x-u_0)(\boldsymbol x-u_0)^T +<br>    \sum_{\boldsymbol x\epsilon X_1}(\boldsymbol x-u_1)(\boldsymbol x-u_1)^T<br>$$<br>类间散度矩阵 $S_b = (u_0 - u_1)(u_0 - u_1)^T$</p><p>据上分析，优化目标为<br>$$<br>\frac{\boldsymbol w^TS_b\boldsymbol w}{\boldsymbol w^TS_w\boldsymbol w}<br>$$</p><p>根据广义瑞利商的性质，矩阵 $S^{-1}<em>{w} S_b$ 的最大特征值为 $J(\boldsymbol w)$ 的最大值，矩阵 $S^{-1}</em>{w} S_b$ 的最大特征值对应的特征向量即为 $\boldsymbol w$。</p><h3 id="2-14-4-LDA算法流程总结"><a href="#2-14-4-LDA算法流程总结" class="headerlink" title="2.14.4 LDA算法流程总结"></a>2.14.4 LDA算法流程总结</h3><p>LDA算法降维流程如下：</p><p>输入：数据集 $D = { (x_1,y_1),(x_2,y_2), … ,(x_m,y_m) }$，其中样本 $x_i $ 是n维向量，$y_i  \epsilon {C_1, C_2, …, C_k}$，降维后的目标维度 $d$ 。</p><p>输出：降维后的数据集 $\overline{D} $ 。</p><p>步骤：</p><ol><li>计算类内散度矩阵 $S_w$。</li><li>计算类间散度矩阵 $S_b$ 。</li><li>计算矩阵 $S^{-1}_wS_b$ 。</li><li>计算矩阵 $S^{-1}_wS_b$ 的最大的 d 个特征值。</li><li>计算 d 个特征值对应的 d 个特征向量，记投影矩阵为 W 。</li><li>转化样本集的每个样本，得到新样本 $P_i = W^Tx_i$ 。</li><li>输出新样本集 $\overline{D} = { (p_1,y_1),(p_2,y_2),…,(p_m,y_m) }$</li></ol><h3 id="2-14-5-LDA和PCA区别"><a href="#2-14-5-LDA和PCA区别" class="headerlink" title="2.14.5 LDA和PCA区别"></a>2.14.5 LDA和PCA区别</h3><table><thead><tr><th align="center">异同点</th><th align="left">LDA</th><th align="left">PCA</th></tr></thead><tbody><tr><td align="center">相同点</td><td align="left">1. 两者均可以对数据进行降维；<br>2. 两者在降维时均使用了矩阵特征分解的思想；<br>3. 两者都假设数据符合高斯分布；</td><td align="left"></td></tr><tr><td align="center">不同点</td><td align="left">有监督的降维方法；</td><td align="left">无监督的降维方法；</td></tr><tr><td align="center"></td><td align="left">降维最多降到k-1维；</td><td align="left">降维多少没有限制；</td></tr><tr><td align="center"></td><td align="left">可以用于降维，还可以用于分类；</td><td align="left">只用于降维；</td></tr><tr><td align="center"></td><td align="left">选择分类性能最好的投影方向；</td><td align="left">选择样本点投影具有最大方差的方向；</td></tr><tr><td align="center"></td><td align="left">更明确，更能反映样本间差异；</td><td align="left">目的较为模糊；</td></tr></tbody></table><h3 id="2-14-6-LDA优缺点"><a href="#2-14-6-LDA优缺点" class="headerlink" title="2.14.6 LDA优缺点"></a>2.14.6 LDA优缺点</h3><table><thead><tr><th align="center">优缺点</th><th align="left">简要说明</th></tr></thead><tbody><tr><td align="center">优点</td><td align="left">1. 可以使用类别的先验知识；<br>2. 以标签、类别衡量差异性的有监督降维方式，相对于PCA的模糊性，其目的更明确，更能反映样本间的差异；</td></tr><tr><td align="center">缺点</td><td align="left">1. LDA不适合对非高斯分布样本进行降维；<br>2. LDA降维最多降到分类数k-1维；<br>3. LDA在样本分类信息依赖方差而不是均值时，降维效果不好；<br>4. LDA可能过度拟合数据。</td></tr></tbody></table><h2 id="2-15-主成分分析（PCA）"><a href="#2-15-主成分分析（PCA）" class="headerlink" title="2.15  主成分分析（PCA）"></a>2.15  主成分分析（PCA）</h2><h3 id="2-15-1-主成分分析（PCA）思想总结"><a href="#2-15-1-主成分分析（PCA）思想总结" class="headerlink" title="2.15.1 主成分分析（PCA）思想总结"></a>2.15.1 主成分分析（PCA）思想总结</h3><ol><li>PCA就是将高维的数据通过线性变换投影到低维空间上去。</li><li>投影思想：找出最能够代表原始数据的投影方法。被PCA降掉的那些维度只能是那些噪声或是冗余的数据。</li><li>去冗余：去除可以被其他向量代表的线性相关向量，这部分信息量是多余的。</li><li>去噪声，去除较小特征值对应的特征向量，特征值的大小反映了变换后在特征向量方向上变换的幅度，幅度越大，说明这个方向上的元素差异也越大，要保留。</li><li>对角化矩阵，寻找极大线性无关组，保留较大的特征值，去除较小特征值，组成一个投影矩阵，对原始样本矩阵进行投影，得到降维后的新样本矩阵。</li><li>完成PCA的关键是——协方差矩阵。协方差矩阵，能同时表现不同维度间的相关性以及各个维度上的方差。协方差矩阵度量的是维度与维度之间的关系，而非样本与样本之间。</li><li>之所以对角化，因为对角化之后非对角上的元素都是0，达到去噪声的目的。对角化后的协方差矩阵，对角线上较小的新方差对应的就是那些该去掉的维度。所以我们只取那些含有较大能量(特征值)的维度，其余的就舍掉，即去冗余。</li></ol><p>但是他们虽然都能实现对数据的约减。但是二者适用的对象不同，聚类针对的是数据点，而降维则是对于数据的特征。另外它们有着很多种实现方法。聚类中常用的有K-means、层次聚类、基于密度的聚类等；降维中常用的则PCA、Isomap、LLE等。</p><h3 id="2-21-4-有哪些聚类算法优劣衡量标准"><a href="#2-21-4-有哪些聚类算法优劣衡量标准" class="headerlink" title="2.21.4 有哪些聚类算法优劣衡量标准"></a>2.21.4 有哪些聚类算法优劣衡量标准</h3><p>不同聚类算法有不同的优劣和不同的适用条件。可从以下方面进行衡量判断：<br>    1、算法的处理能力：处理大的数据集的能力，即算法复杂度；处理数据噪声的能力；处理任意形状，包括有间隙的嵌套的数据的能力；<br>    2、算法是否需要预设条件：是否需要预先知道聚类个数，是否需要用户给出领域知识； </p><p>3、算法的数据输入属性：算法处理的结果与数据输入的顺序是否相关，也就是说算法是否独立于数据输入顺序；算法处理有很多属性数据的能力，也就是对数据维数是否敏感，对数据的类型有无要求。</p><h3 id="2-21-5-聚类和分类有什么区别"><a href="#2-21-5-聚类和分类有什么区别" class="headerlink" title="2.21.5 聚类和分类有什么区别"></a>2.21.5 聚类和分类有什么区别</h3><p>*<em>聚类（Clustering） *</em><br>    聚类，简单地说就是把相似的东西分到一组，聚类的时候，我们并不关心某一类是什么，我们需要实现的目标只是把相似的东西聚到一起。一个聚类算法通常只需要知道如何计算相似度就可以开始工作了，因此聚类通常并不需要使用训练数据进行学习，在机器学习中属于无监督学习。 </p><p>*<em>分类（Classification） *</em></p><p> 分类，对于一个分类器，通常需要你告诉它“这个东西被分为某某类”。一般情况下，一个分类器会从它得到的训练集中进行学习，从而具备对未知数据进行分类的能力，在机器学习中属于监督学习。</p><h3 id="2-21-6-不同聚类算法特点性能比较"><a href="#2-21-6-不同聚类算法特点性能比较" class="headerlink" title="2.21.6 不同聚类算法特点性能比较"></a>2.21.6 不同聚类算法特点性能比较</h3><table><thead><tr><th align="center">算法名称</th><th align="center">可伸缩性</th><th align="center">适合的数据类型</th><th align="center">高维性</th><th align="center">异常数据抗干扰性</th><th align="center">聚类形状</th><th align="center">算法效率</th></tr></thead><tbody><tr><td align="center">WAVECLUSTER</td><td align="center">很高</td><td align="center">数值型</td><td align="center">很高</td><td align="center">较高</td><td align="center">任意形状</td><td align="center">很高</td></tr><tr><td align="center">ROCK</td><td align="center">很高</td><td align="center">混合型</td><td align="center">很高</td><td align="center">很高</td><td align="center">任意形状</td><td align="center">一般</td></tr><tr><td align="center">BIRCH</td><td align="center">较高</td><td align="center">数值型</td><td align="center">较低</td><td align="center">较低</td><td align="center">球形</td><td align="center">很高</td></tr><tr><td align="center">CURE</td><td align="center">较高</td><td align="center">数值型</td><td align="center">一般</td><td align="center">很高</td><td align="center">任意形状</td><td align="center">较高</td></tr><tr><td align="center">K-PROTOTYPES</td><td align="center">一般</td><td align="center">混合型</td><td align="center">较低</td><td align="center">较低</td><td align="center">任意形状</td><td align="center">一般</td></tr><tr><td align="center">DENCLUE</td><td align="center">较低</td><td align="center">数值型</td><td align="center">较高</td><td align="center">一般</td><td align="center">任意形状</td><td align="center">较高</td></tr><tr><td align="center">OPTIGRID</td><td align="center">一般</td><td align="center">数值型</td><td align="center">较高</td><td align="center">一般</td><td align="center">任意形状</td><td align="center">一般</td></tr><tr><td align="center">CLIQUE</td><td align="center">较高</td><td align="center">数值型</td><td align="center">较高</td><td align="center">较高</td><td align="center">任意形状</td><td align="center">较低</td></tr><tr><td align="center">DBSCAN</td><td align="center">一般</td><td align="center">数值型</td><td align="center">较低</td><td align="center">较高</td><td align="center">任意形状</td><td align="center">一般</td></tr><tr><td align="center">CLARANS</td><td align="center">较低</td><td align="center">数值型</td><td align="center">较低</td><td align="center">较高</td><td align="center">球形</td><td align="center">较低</td></tr></tbody></table><h3 id="2-21-7-四种常用聚类方法之比较"><a href="#2-21-7-四种常用聚类方法之比较" class="headerlink" title="2.21.7 四种常用聚类方法之比较"></a>2.21.7 四种常用聚类方法之比较</h3><p>聚类就是按照某个特定标准把一个数据集分割成不同的类或簇，使得同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大。即聚类后同一类的数据尽可能聚集到一起，不同类数据尽量分离。<br>主要的聚类算法可以划分为如下几类：划分方法、层次方法、基于密度的方法、基于网格的方法以及基于模型的方法。下面主要对k-means聚类算法、凝聚型层次聚类算法、神经网络聚类算法之SOM,以及模糊聚类的FCM算法通过通用测试数据集进行聚类效果的比较和分析。</p><h3 id="2-21-8-k-means聚类算法"><a href="#2-21-8-k-means聚类算法" class="headerlink" title="2.21.8 k-means聚类算法"></a>2.21.8 k-means聚类算法</h3><p>k-means是划分方法中较经典的聚类算法之一。由于该算法的效率高，所以在对大规模数据进行聚类时被广泛应用。目前，许多算法均围绕着该算法进行扩展和改进。<br>k-means算法以k为参数，把n个对象分成k个簇，使簇内具有较高的相似度，而簇间的相似度较低。k-means算法的处理过程如下：首先，随机地 选择k个对象，每个对象初始地代表了一个簇的平均值或中心;对剩余的每个对象，根据其与各簇中心的距离，将它赋给最近的簇;然后重新计算每个簇的平均值。 这个过程不断重复，直到准则函数收敛。通常，采用平方误差准则，其定义如下：<br>$$<br>E=\sum_{i=1}^{k}\sum_{p\in C_i}\left|p-m_i\right|^2<br>$$<br>　这里E是数据中所有对象的平方误差的总和，p是空间中的点，$m_i$是簇$C_i$的平均值[9]。该目标函数使生成的簇尽可能紧凑独立，使用的距离度量是欧几里得距离，当然也可以用其他距离度量。</p><p><strong>算法流程</strong>：<br>    输入：包含n个对象的数据和簇的数目k；<br>    输出：n个对象到k个簇，使平方误差准则最小。<br>    步骤：<br>　　(1) 任意选择k个对象作为初始的簇中心；<br>　　(2) 根据簇中对象的平均值，将每个对象(重新)赋予最类似的簇；<br>　　(3) 更新簇的平均值，即计算每个簇中对象的平均值；<br>　　(4) 重复步骤(2)、(3)直到簇中心不再变化；</p><h3 id="2-21-9-层次聚类算法"><a href="#2-21-9-层次聚类算法" class="headerlink" title="2.21.9 层次聚类算法"></a>2.21.9 层次聚类算法</h3><p>　凝聚型层次聚类的策略是先将每个对象作为一个簇，然后合并这些原子簇为越来越大的簇，直到所有对象都在一个簇中，或者某个终结条件被满足。绝大多数层次聚类属于凝聚型层次聚类，它们只是在簇间相似度的定义上有所不同。</p><p><strong>算法流程</strong>：</p><p>注：以采用最小距离的凝聚层次聚类算法为例：</p><p>　(1) 将每个对象看作一类，计算两两之间的最小距离；<br>　(2) 将距离最小的两个类合并成一个新类；<br>　(3) 重新计算新类与所有类之间的距离；<br>　(4) 重复(2)、(3)，直到所有类最后合并成一类。</p><h3 id="2-21-10-SOM聚类算法"><a href="#2-21-10-SOM聚类算法" class="headerlink" title="2.21.10 SOM聚类算法"></a>2.21.10 SOM聚类算法</h3><p>SOM神经网络[11]是由芬兰神经网络专家Kohonen教授提出的，该算法假设在输入对象中存在一些拓扑结构或顺序，可以实现从输入空间(n维)到输出平面(2维)的降维映射，其映射具有拓扑特征保持性质,与实际的大脑处理有很强的理论联系。</p><p>SOM网络包含输入层和输出层。输入层对应一个高维的输入向量，输出层由一系列组织在2维网格上的有序节点构成，输入节点与输出节点通过权重向量连接。 学习过程中，找到与之距离最短的输出层单元，即获胜单元，对其更新。同时，将邻近区域的权值更新，使输出节点保持输入向量的拓扑特征。</p><p><strong>算法流程</strong>：</p><p>(1) 网络初始化，对输出层每个节点权重赋初值；<br>(2) 从输入样本中随机选取输入向量并且归一化，找到与输入向量距离最小的权重向量；<br>(3) 定义获胜单元，在获胜单元的邻近区域调整权重使其向输入向量靠拢；<br>(4) 提供新样本、进行训练；<br>(5) 收缩邻域半径、减小学习率、重复，直到小于允许值，输出聚类结果。</p><h3 id="2-21-11-FCM聚类算法"><a href="#2-21-11-FCM聚类算法" class="headerlink" title="2.21.11 FCM聚类算法"></a>2.21.11 FCM聚类算法</h3><p>1965年美国加州大学柏克莱分校的扎德教授第一次提出了‘集合’的概念。经过十多年的发展，模糊集合理论渐渐被应用到各个实际应用方面。为克服非此即彼的分类缺点，出现了以模糊集合论为数学基础的聚类分析。用模糊数学的方法进行聚类分析，就是模糊聚类分析[12]。<br>FCM算法是一种以隶属度来确定每个数据点属于某个聚类程度的算法。该聚类算法是传统硬聚类算法的一种改进。<br>设数据集$X={x_1,x_2,…,x_n}$,它的模糊$c$划分可用模糊矩阵$U=[u_{ij}]$表示，矩阵$U$的元素$u_{ij}$表示第$j(j=1,2,…,n)$个数据点属于第$i(i=1,2,…,c)$类的隶属度，$u_{ij}$满足如下条件：<br>$$<br>\begin{equation}<br>\left{<br>\begin{array}{lr}<br>\sum_{i=1}^c u_{ij}=1 \quad\forall~j<br>\u_{ij}\in[0,1] \quad\forall ~i,j<br>\\sum_{j=1}^c u_{ij}&gt;0 \quad\forall ~i<br>\end{array}<br>\right.<br>\end{equation}<br>$$<br>目前被广泛使用的聚类准则是取类内加权误差平方和的极小值。即：<br>$$<br>(min)J_m(U,V)=\sum^n_{j=1}\sum^c_{i=1}u^m_{ij}d^2_{ij}(x_j,v_i)<br>$$<br>其中$V$为聚类中心，$m$为加权指数，$d_{ij}(x_j,v_i)=||v_i-x_j||$。</p><p><strong>算法流程</strong>：</p><p>　(1) 标准化数据矩阵；<br>　(2) 建立模糊相似矩阵，初始化隶属矩阵；<br>　(3) 算法开始迭代，直到目标函数收敛到极小值；<br>　(4) 根据迭代结果，由最后的隶属矩阵确定数据所属的类，显示最后的聚类结果。</p><h3 id="2-21-12-四种聚类算法试验"><a href="#2-21-12-四种聚类算法试验" class="headerlink" title="2.21.12 四种聚类算法试验"></a>2.21.12 四种聚类算法试验</h3><p>选取专门用于测试分类、聚类算法的国际通用的UCI数据库中的IRIS数据集，IRIS数据集包含150个样本数据，分别取自三种不同 的莺尾属植物setosa、versicolor和virginica的花朵样本,每个数据含有4个属性，即萼片长度、萼片宽度、花瓣长度、花瓣宽度，单位为cm。 在数据集上执行不同的聚类算法，可以得到不同精度的聚类结果。基于前面描述的各算法原理及流程，可初步得如下聚类结果。</p><table><thead><tr><th>聚类方法</th><th>聚错样本数</th><th>运行时间/s</th><th>平均准确率/（%）</th></tr></thead><tbody><tr><td>K-means</td><td>17</td><td>0.146001</td><td>89</td></tr><tr><td>层次聚类</td><td>51</td><td>0.128744</td><td>66</td></tr><tr><td>SOM</td><td>22</td><td>5.267283</td><td>86</td></tr><tr><td>FCM</td><td>12</td><td>0.470417</td><td>92</td></tr></tbody></table><p><strong>注</strong>：</p><p>(1) 聚错样本数：总的聚错的样本数，即各类中聚错的样本数的和；<br>(2) 运行时间：即聚类整个过程所耗费的时间，单位为s；<br>(3) 平均准确度：设原数据集有k个类,用$c_i$表示第i类，$n_i$为$c_i$中样本的个数，$m_i$为聚类正确的个数,则$m_i/n_i$为 第i类中的精度，则平均精度为：$avg=\frac{1}{k}\sum_{i=1}^{k}\frac{m_{i}}{n_{i}}$。  </p><h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p>给定一个整数的数组nums，返回相加为target的两个数字的索引值。</p><p>假设每次输入都只有一个答案，并且不会使用同一个元素两次。</p><p><strong>举例：</strong></p><pre class="line-numbers language-stylus"><code class="language-stylus"><span class="token selector">Given nums = [2<span class="token punctuation">,</span> 7<span class="token punctuation">,</span> 11<span class="token punctuation">,</span> 15]<span class="token punctuation">,</span> target = 9<span class="token punctuation">,</span></span><span class="token selector">Because nums[0] + nums[1] = 2 + 7 = 9<span class="token punctuation">,</span></span><span class="token statement"><span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">.</span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h1 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h1><p>最初，我的解题思路是最简单的遍历，如果数组nums的元素a小于target，那么就在数组中寻找另外一个b，使a+b=target。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">twoSum</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> nums<span class="token punctuation">,</span> target<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        :type nums: List[int]        :type target: int        :rtype: List[int]        """</span>        result <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> i<span class="token punctuation">,</span> each <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>nums<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> abs<span class="token punctuation">(</span>target<span class="token operator">-</span>each<span class="token punctuation">)</span> <span class="token operator">>=</span><span class="token number">0</span> <span class="token operator">and</span> i <span class="token operator">not</span> <span class="token keyword">in</span> result<span class="token punctuation">:</span>                <span class="token keyword">try</span><span class="token punctuation">:</span>                    tmp <span class="token operator">=</span> nums<span class="token punctuation">.</span>index<span class="token punctuation">(</span>target <span class="token operator">-</span> each<span class="token punctuation">)</span>                    <span class="token keyword">if</span> tmp <span class="token operator">!=</span> i<span class="token punctuation">:</span>                        result<span class="token punctuation">.</span>append<span class="token punctuation">(</span>i<span class="token punctuation">)</span>                        result<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tmp<span class="token punctuation">)</span>                <span class="token keyword">except</span><span class="token punctuation">:</span>                    <span class="token keyword">continue</span>        <span class="token keyword">return</span> result<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>运行通过，但是运行速度特别慢！Beats才20%+。list的index操作时间复杂度为O(1)，list的append操作时间复杂度也为O(1)。list的not in时间复杂度为O(n)，在算上循环，总共时间复杂度为O(n^2)？所以才这么慢吧….</p><p>继续想一些更高级的解决办法。</p><p>使用哈希表，也就是散列表，在Python就是字典。使用方法很巧妙，直接看代码吧！</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">twoSum</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> nums<span class="token punctuation">,</span> target<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        :type nums: List[int]        :type target: int        :rtype: List[int]        """</span>        <span class="token keyword">if</span> len<span class="token punctuation">(</span>nums<span class="token punctuation">)</span> <span class="token operator">&lt;=</span> <span class="token number">1</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> <span class="token boolean">False</span>        buff_dict <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>nums<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">in</span> buff_dict<span class="token punctuation">:</span>                <span class="token keyword">return</span> <span class="token punctuation">[</span>buff_dict<span class="token punctuation">[</span>nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                buff_dict<span class="token punctuation">[</span>target <span class="token operator">-</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> i<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Beats 90%+。</p><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="//youwebsite.com/2020/04/29/ji-qi-xue-xi-one/2.16.20.1-1588173998266.png" alt="2.16.20.1"></p> <img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="/2020/04/29/ji-qi-xue-xi-one/2.16.20.1.png" title="图像名字"><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]   Goodfellow I, Bengio Y, Courville A. Deep learning[M]. MIT press, 2016.<br>[2]   周志华. 机器学习[M].清华大学出版社, 2016.<br>[3]   Michael A. Nielsen. “Neural Networks and Deep Learning”, Determination Press, 2015.<br>[4]   Suryansh S. Gradient Descent: All You Need to Know, 2018.<br>[5]   刘建平. 梯度下降小结,EM算法的推导, 2018<br>[6]   杨小兵．聚类分析中若干关键技术的研究[D]． 杭州：浙江大学, 2005.<br>[7]   XU Rui, Donald Wunsch 1 1． survey of clustering algorithm[J]．IEEE．Transactions on Neural Networks, 2005, 16(3)：645-67 8.<br>[8]   YI Hong, SAM K． Learning assignment order of instances for the constrained k-means clustering algorithm[J]．IEEE Transactions on Systems, Man, and Cybernetics, Part B：Cybernetics,2009,39 (2)：568-574.<br>[9]   贺玲, 吴玲达, 蔡益朝．数据挖掘中的聚类算法综述[J]．计算机应用研究, 2007, 24(1):10-13．<br>[10]  孙吉贵, 刘杰, 赵连宇．聚类算法研究[J]．软件学报, 2008, 19(1)：48-61．<br>[11]  孔英会, 苑津莎, 张铁峰等．基于数据流管理技术的配变负荷分类方法研究．中国国际供电会议, CICED2006．<br>[12]  马晓艳, 唐雁．层次聚类算法研究[J]．计算机科学, 2008, 34(7)：34-36．<br>[13]  FISHER R A． Iris Plants Database <a href="https://www.ics.uci.edu/vmlearn/MLRepository.html" target="_blank" rel="noopener">https://www.ics.uci.edu/vmlearn/MLRepository.html</a>, Authorized license．<br>[14]  Quinlan J R. Induction of decision trees[J]. Machine learning, 1986, 1(1): 81-106.<br>[15]  Breiman L. Random forests[J]. Machine learning, 2001, 45(1): 5-32.  </p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>To Jane 521</title>
      <link href="/2020/04/29/to-jane-521/"/>
      <url>/2020/04/29/to-jane-521/</url>
      
        <content type="html"><![CDATA[<h2 id="😍致最好的宝💖"><a href="#😍致最好的宝💖" class="headerlink" title="😍致最好的宝💖"></a>😍致最好的宝💖</h2><p>第一次做个人博客，first内容当然要献给宝儿💏，一直想着要为我宝来证明肉肉爱意的，先来几个小句子热热身吧</p><ul><li><input disabled="" type="checkbox"> 🌊⬆🈷🔟👅⬆🈷        🚬💰人🔟🆕⬆人</li><li><input disabled="" type="checkbox"> 1️⃣带➖宽🀄8️⃣悔，为1️⃣😁🉐🙎‍♂️🌉悴</li><li><input disabled="" type="checkbox"> 🐵🚪1️⃣入🈸4️⃣海，从🧲😁郎🔟路人</li><li><input disabled="" type="checkbox"> ⭕🉐1️⃣🆕人，白头8️⃣🐘🎁</li></ul><p>😉听完肉肉的以上表白是不是心里特别的清爽和有感觉呢？仙女宝宝😍，肉肉那天确实是失态了，肉肉知道那种我当时的情景已经完全把宝宝给吓坏了，也才会有宝宝心里想的傻傻的想法，说什么肉肉是一直在忍受，宝宝自己脾气不好。可是事实真的根本不是这样的，对不起是肉肉的表现让宝宝产生了这种错觉😥。</p><p>$$\left[-5 e^{2 i \pi} +\frac{1}{3} \right] / 2=\frac{1}{4}$$</p><h4 id="😚肉肉对宝宝的爱就像公式里阐明的那样，永远不会变。"><a href="#😚肉肉对宝宝的爱就像公式里阐明的那样，永远不会变。" class="headerlink" title="😚肉肉对宝宝的爱就像公式里阐明的那样，永远不会变。"></a>😚肉肉对宝宝的爱就像公式里阐明的那样，永远不会变。</h4><p>​                                                        我💓你</p><p>​                                                🈚畏人海💧拥📝</p><p>​                                                用🈲🌧️生💧勇7️⃣</p><p>​                                        📄为能靠🈲你🌶️爪巴1️⃣🎁㊙</p><p>​                                                        💓⬆你</p><p>​                                                🔟我🌶️⬇💧🧵7️⃣</p><p>​                                                8️⃣惧岁🈷💧更替</p><p>​                                                    往🐵💧朝夕</p><p>​                                                    8️⃣论🐝🌧️</p><p>​                                                    🔟你9️⃣🦶1️⃣</p><h2 id="💓肉肉对宝宝的心就像这首歌，永远那么深情。"><a href="#💓肉肉对宝宝的心就像这首歌，永远那么深情。" class="headerlink" title="💓肉肉对宝宝的心就像这首歌，永远那么深情。"></a>💓肉肉对宝宝的心就像这首歌，永远那么深情。</h2><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客 </tag>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客开源</title>
      <link href="/2019/08/27/blog-open-source/"/>
      <url>/2019/08/27/blog-open-source/</url>
      
        <content type="html"><![CDATA[<h1 id="效果演示"><a href="#效果演示" class="headerlink" title="效果演示"></a>效果演示</h1><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://raw.githubusercontent.com/shw2018/cdn/master/blog_files/img/Blog-Open-Source/blog-demo1.gif" alt="演示Demo"></p><h1 id="简短介绍"><a href="#简短介绍" class="headerlink" title="简短介绍"></a>简短介绍</h1><p>个人博客网站完善了，目前这个版本使用应该是够了，当然还有一些优化项和功能增加后续在慢慢更新，为了回馈开源，可以直接使用的源码，您只需要把博客相关信息换成您自己的就可以部署了，对于新手或者不懂编程的小伙伴来说，简直是福音，极大简化了您构建博客的工作量和复杂度，每个人都可以下载并修改成自己喜欢样式！如果你有修改想法，欢迎PR！最后，我们还是给这个开源小项目取个名字吧，就叫<a href="https://github.com/shw2018/hexo-blog-fly.git" target="_blank" rel="noopener">hexo-blog-fly</a>吧，怎么样？&lt;&lt;&lt;&lt;&lt;<a href="https://github.com/shw2018/hexo-blog-fly" target="_blank" rel="noopener">源代码下载</a>&gt;&gt;&gt;&gt;&gt;</p><p>本博客基于<code>Hexo</code>框架搭建，用到<a href="https://github.com/shw2018/hexo-theme-matery" target="_blank" rel="noopener">hexo-theme-matery</a>主题, 并在此基础之上做了很多修改，修复了一些bug，增加了一些新的特性和功能，博客地址：<a href="https://shw2018.github.io/" target="_blank" rel="noopener">https://shw2018.github.io</a>，博客演示：<a href="https://sunhwee.com" target="_blank" rel="noopener">sunhwee.com</a>。</p><hr><h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><p><strong>原主题特性</strong>:</p><ul><li><p>简单漂亮，文章内容美观易读</p></li><li><p><a href="https://material.io/" target="_blank" rel="noopener">Material Design</a> 设计</p></li><li><p>响应式设计，博客在桌面端、平板、手机等设备上均能很好的展现</p></li><li><p>首页轮播文章及每天动态切换 <code>Banner</code> 图片</p></li><li><p>瀑布流式的博客文章列表（文章无特色图片时会有 <code>24</code> 张漂亮的图片代替）</p></li><li><p>时间轴式的归档页</p></li><li><p><strong>词云</strong>的标签页和<strong>雷达图</strong>的分类页</p></li><li><p>丰富的关于我页面（包括关于我、文章统计图、我的项目、我的技能、相册等）</p></li><li><p>可自定义的数据的友情链接页面</p></li><li><p>支持文章置顶和文章打赏</p></li><li><p>支持 <code>MathJax</code></p></li><li><p><code>TOC</code> 目录</p></li><li><p>可设置复制文章内容时追加版权信息</p></li><li><p>可设置阅读文章时做密码验证</p></li><li><p><a href="https://gitalk.github.io/" target="_blank" rel="noopener">Gitalk</a>、<a href="https://imsun.github.io/gitment/" target="_blank" rel="noopener">Gitment</a>、<a href="https://valine.js.org/" target="_blank" rel="noopener">Valine</a> 和 <a href="https://disqus.com/" target="_blank" rel="noopener">Disqus</a> 评论模块（推荐使用 <code>Gitalk</code>）</p></li><li><p>集成了<a href="http://busuanzi.ibruce.info/" target="_blank" rel="noopener">不蒜子统计</a>、谷歌分析（<code>Google Analytics</code>）和文章字数统计等功能</p></li><li><p>支持在首页的音乐播放和视频播放功能</p><p><strong>增加的工作或特性(未打钩的是已做但还没更新到源码的)</strong>:</p></li><li><p>修改了原主题的一些很多<code>bug</code>   2019.08.05</p></li><li><p>加入图片懒加载功能，在根目录配置文件开启和关闭    2019.08.09</p></li><li><p>增加<code>留言板</code>功能          2019.08.05</p></li><li><p>在关于板块,加入<code>简历</code>功能页   2019.08.05</p></li><li><p>增加视听[视觉听觉影音]板块       2019.08.10</p></li><li><p>支持<code>emoji</code>表情，用<code>markdown emoji</code>语法书写直接生成对应的能<strong>跳跃</strong>的表情。  2019.08.10</p></li><li><p>增加网站运行时间显示  2019.08.10</p></li><li><p>增加<code>动漫模型</code>     2019.08.10</p></li><li><p>整体替换Banner图片和文章特色图片   2019.08.10</p></li><li><p>增加分类<code>相册</code>功能         2019.08.29</p></li><li><p>去掉标签页,将其合并至<code>分类</code>页中                2019.09.01</p></li><li><p>修改了一些控件的参数   2019.09.01</p></li><li><p>修改部分样式,比如: 文章卡片,固定高度,使其不至于因为文章摘要的长短不同导致卡片大小不一使页面布局很不美观,类似的还有友链卡片,优化了页面内容布局,视觉更整齐美观          2019.09.01</p></li><li><p>解决首页文章列表卡片上方 <code>border-radius</code>圆角失效的bug  2019.09.01</p></li><li><p>添加页面樱花飘落动效            2019.09.09</p></li><li><p>添加鼠标点击烟花爆炸动效   2019.09.09</p></li><li><p>加入天气接口控件   2019.09.09</p></li><li><p>加入鼠标点击文字特效   2019.09.10</p></li><li><p>添加页面雪花飘落动效            2019.09.10</p></li><li><p>添加在线聊天插件            2019.09.12</p></li><li><p>持续更新…</p></li></ul><hr><p><strong>简单使用方法：</strong></p><ol><li><code>star</code> 上述项目</li><li>安装<a href="https://git-scm.com/downloads" target="_blank" rel="noopener">Git</a>, 安装<a href="https://nodejs.org/en/" target="_blank" rel="noopener">nodeJS</a></li><li>你可以直接<code>fork</code>一份源码到你的仓库，<code>clone</code>到本地</li><li>在本地博客仓库运行<code>npm i</code>命令安装依赖包</li><li>修改配置信息，改成自己的信息</li><li>运行命令<code>hexo  clean</code>（清除生成文件），<code>hexo g</code>（生成网页）， <code>hexo  s</code>（本地预览），<code>hexo d</code>（部署）</li></ol><blockquote><p><strong>更多详情教程，强烈推荐：<a href="https://sunhwee.com/posts/6e8839eb.html" target="_blank" rel="noopener">Hexo+Github博客搭建完全教程</a></strong></p></blockquote><blockquote><p><strong>最后，如果项目和教程对你有所帮助或者你看见了还算比较喜欢，欢迎给上述作者<code>star</code>，谢谢您！</strong></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 软件安装与配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Github </tag>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
